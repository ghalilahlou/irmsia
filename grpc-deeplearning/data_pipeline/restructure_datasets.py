"""
Restructure Datasets - Organisation intelligente pour training efficace
Trie, organise et pr√©pare les datasets pour un training optimal
"""

import sys
import io
from pathlib import Path
import pandas as pd
import shutil
from typing import Dict, List, Tuple
import logging
from tqdm import tqdm
import json

# Fix Windows encoding
if sys.platform == 'win32':
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')
    except:
        pass

logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)


class DatasetRestructurer:
    """
    Restructure et organise les datasets pour training efficace
    """
    
    def __init__(self, base_dir: str = "datasets"):
        self.base_dir = Path(base_dir)
        self.organized_dir = self.base_dir / "organized"
        self.organized_dir.mkdir(parents=True, exist_ok=True)
        
    def analyze_brain_mri_dataset(self):
        """
        Analyser le dataset Brain MRI et cr√©er des classes √† partir des masques
        """
        print("\n" + "="*70)
        print("ANALYSE DU DATASET BRAIN MRI")
        print("="*70)
        
        brain_mri_dir = self.base_dir / "kaggle" / "mateuszbuda_lgg-mri-segmentation" / "lgg-mri-segmentation" / "kaggle_3m"
        
        if not brain_mri_dir.exists():
            print(f"‚ùå Dataset Brain MRI non trouv√©: {brain_mri_dir}")
            return None
        
        print(f"\nüìÇ R√©pertoire: {brain_mri_dir}")
        
        # Lister tous les patients
        patient_dirs = [d for d in brain_mri_dir.iterdir() if d.is_dir() and d.name.startswith('TCGA')]
        
        print(f"üìä Patients trouv√©s: {len(patient_dirs)}")
        
        # Analyser chaque patient
        images_with_mask = 0
        images_without_mask = 0
        
        structured_data = []
        
        for patient_dir in tqdm(patient_dirs, desc="Analyse patients"):
            patient_id = patient_dir.name
            
            # Trouver toutes les images
            images = list(patient_dir.glob("*.tif"))
            
            for img_path in images:
                img_name = img_path.name
                
                # V√©rifier si c'est un mask (contient "_mask")
                if "_mask" in img_name:
                    continue
                
                # Chercher le mask correspondant
                mask_name = img_name.replace(".tif", "_mask.tif")
                mask_path = patient_dir / mask_name
                
                has_tumor = mask_path.exists()
                
                structured_data.append({
                    'patient_id': patient_id,
                    'image_path': str(img_path.relative_to(self.base_dir)),
                    'mask_path': str(mask_path.relative_to(self.base_dir)) if has_tumor else None,
                    'has_tumor': 1 if has_tumor else 0,
                    'label': 'tumor' if has_tumor else 'normal'
                })
                
                if has_tumor:
                    images_with_mask += 1
                else:
                    images_without_mask += 1
        
        print(f"\nüìà Statistiques:")
        print(f"   Images avec tumeur (mask): {images_with_mask}")
        print(f"   Images sans tumeur: {images_without_mask}")
        print(f"   Total: {images_with_mask + images_without_mask}")
        
        # Cr√©er DataFrame
        df = pd.DataFrame(structured_data)
        
        # Sauvegarder
        output_file = self.organized_dir / "brain_mri_structured.csv"
        df.to_csv(output_file, index=False)
        
        print(f"\n‚úÖ Dataset structur√© sauvegard√©: {output_file}")
        
        return df
    
    def organize_by_class(self, csv_file: str, output_name: str):
        """
        Organiser un dataset par classes dans des dossiers s√©par√©s
        Structure: organized/dataset_name/train/class_0/, train/class_1/, val/, test/
        """
        print(f"\n" + "="*70)
        print(f"ORGANISATION PAR CLASSES: {output_name}")
        print("="*70)
        
        # Charger le CSV
        df = pd.read_csv(csv_file)
        
        print(f"\nüìä Dataset:")
        print(f"   Total images: {len(df)}")
        
        # Compter les classes
        if 'label' in df.columns:
            class_counts = df['label'].value_counts()
            print(f"\nüìà Distribution des classes:")
            for label, count in class_counts.items():
                print(f"   {label}: {count} ({count/len(df)*100:.1f}%)")
        elif 'has_tumor' in df.columns:
            class_counts = df['has_tumor'].value_counts()
            print(f"\nüìà Distribution:")
            print(f"   Avec tumeur: {class_counts.get(1, 0)}")
            print(f"   Sans tumeur: {class_counts.get(0, 0)}")
        
        # Cr√©er les splits
        from sklearn.model_selection import train_test_split
        
        # Stratify by label
        stratify_col = df['label'] if 'label' in df.columns else df['has_tumor']
        
        # Train (70%), Temp (30%)
        train_df, temp_df = train_test_split(
            df, 
            test_size=0.3, 
            random_state=42,
            stratify=stratify_col
        )
        
        # Val (15%), Test (15%)
        val_df, test_df = train_test_split(
            temp_df,
            test_size=0.5,
            random_state=42,
            stratify=temp_df[stratify_col.name]
        )
        
        print(f"\n‚úÇÔ∏è  Splits cr√©√©s:")
        print(f"   Train: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)")
        print(f"   Val: {len(val_df)} ({len(val_df)/len(df)*100:.1f}%)")
        print(f"   Test: {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)")
        
        # Cr√©er la structure de dossiers
        organized_dataset_dir = self.organized_dir / output_name
        
        for split_name, split_df in [('train', train_df), ('val', val_df), ('test', test_df)]:
            split_dir = organized_dataset_dir / split_name
            
            # Cr√©er dossiers par classe
            classes = split_df[stratify_col.name].unique()
            for class_name in classes:
                class_dir = split_dir / str(class_name)
                class_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"\nüìÅ Structure cr√©√©e: {organized_dataset_dir}")
        
        # Copier ou cr√©er des liens symboliques
        print(f"\nüìã Cr√©ation de manifestes CSV (sans copie de fichiers)...")
        
        # Sauvegarder les splits
        splits_dir = organized_dataset_dir / "splits"
        splits_dir.mkdir(parents=True, exist_ok=True)
        
        train_df.to_csv(splits_dir / "train.csv", index=False)
        val_df.to_csv(splits_dir / "val.csv", index=False)
        test_df.to_csv(splits_dir / "test.csv", index=False)
        
        print(f"   ‚úÖ train.csv: {len(train_df)} images")
        print(f"   ‚úÖ val.csv: {len(val_df)} images")
        print(f"   ‚úÖ test.csv: {len(test_df)} images")
        
        # Cr√©er un fichier de metadata
        metadata = {
            'dataset_name': output_name,
            'total_images': len(df),
            'num_classes': len(classes),
            'classes': list(map(str, classes)),
            'splits': {
                'train': len(train_df),
                'val': len(val_df),
                'test': len(test_df)
            },
            'class_distribution': class_counts.to_dict() if hasattr(class_counts, 'to_dict') else {}
        }
        
        with open(organized_dataset_dir / "metadata.json", 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"\n‚úÖ Dataset organis√©: {organized_dataset_dir}")
        
        return organized_dataset_dir
    
    def create_balanced_subset(self, csv_file: str, output_name: str, samples_per_class: int = 1000):
        """
        Cr√©er un subset √©quilibr√© d'un dataset
        """
        print(f"\n" + "="*70)
        print(f"CR√âATION SUBSET √âQUILIBR√â: {output_name}")
        print(f"√âchantillons par classe: {samples_per_class}")
        print("="*70)
        
        df = pd.read_csv(csv_file)
        
        # Identifier la colonne de classe
        class_col = 'label' if 'label' in df.columns else 'has_tumor'
        
        # √âchantillonner par classe
        balanced_dfs = []
        
        for class_value in df[class_col].unique():
            class_df = df[df[class_col] == class_value]
            
            # Prendre au maximum samples_per_class
            n_samples = min(len(class_df), samples_per_class)
            sampled = class_df.sample(n=n_samples, random_state=42)
            balanced_dfs.append(sampled)
            
            print(f"   Classe {class_value}: {n_samples} √©chantillons")
        
        # Combiner
        balanced_df = pd.concat(balanced_dfs, ignore_index=True)
        
        # M√©langer
        balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)
        
        print(f"\nüìä Subset √©quilibr√©:")
        print(f"   Total: {len(balanced_df)} images")
        
        # Sauvegarder
        output_file = self.organized_dir / f"{output_name}_balanced.csv"
        balanced_df.to_csv(output_file, index=False)
        
        print(f"\n‚úÖ Subset sauvegard√©: {output_file}")
        
        return balanced_df
    
    def list_organized_datasets(self):
        """
        Lister tous les datasets organis√©s
        """
        print("\n" + "="*70)
        print("DATASETS ORGANIS√âS")
        print("="*70)
        
        if not self.organized_dir.exists():
            print("\n‚ùå Aucun dataset organis√© trouv√©")
            return
        
        datasets = [d for d in self.organized_dir.iterdir() if d.is_dir()]
        
        if not datasets:
            print("\n‚ùå Aucun dataset organis√© trouv√©")
            return
        
        print(f"\nüìÇ {len(datasets)} dataset(s) organis√©(s):\n")
        
        for dataset_dir in datasets:
            metadata_file = dataset_dir / "metadata.json"
            
            if metadata_file.exists():
                with open(metadata_file, 'r') as f:
                    metadata = json.load(f)
                
                print(f"   üîπ {dataset_dir.name}")
                print(f"      Images: {metadata['total_images']}")
                print(f"      Classes: {metadata['num_classes']} ({', '.join(metadata['classes'])})")
                print(f"      Splits: Train={metadata['splits']['train']}, Val={metadata['splits']['val']}, Test={metadata['splits']['test']}")
                print()
            else:
                print(f"   üîπ {dataset_dir.name} (pas de metadata)")
                print()


def main():
    """
    Menu interactif pour restructuration de datasets
    """
    
    print("\n" + "="*70)
    print("RESTRUCTURATION DE DATASETS POUR TRAINING EFFICACE")
    print("="*70)
    
    restructurer = DatasetRestructurer()
    
    while True:
        print("\n" + "="*70)
        print("MENU PRINCIPAL")
        print("="*70)
        print("\n1. Analyser et structurer Brain MRI (avec d√©tection tumeurs)")
        print("2. Organiser un dataset par classes")
        print("3. Cr√©er un subset √©quilibr√©")
        print("4. Lister les datasets organis√©s")
        print("5. Quitter")
        
        choice = input("\nVotre choix (1-5): ").strip()
        
        if choice == '1':
            # Brain MRI
            df = restructurer.analyze_brain_mri_dataset()
            
            if df is not None:
                # Organiser automatiquement
                proceed = input("\n‚û°Ô∏è  Organiser ce dataset maintenant? (y/n): ").strip().lower()
                if proceed == 'y':
                    restructurer.organize_by_class(
                        "datasets/organized/brain_mri_structured.csv",
                        "brain_mri"
                    )
        
        elif choice == '2':
            # Organiser un dataset
            csv_file = input("\nChemin du fichier CSV: ").strip()
            output_name = input("Nom du dataset organis√©: ").strip()
            
            if Path(csv_file).exists():
                restructurer.organize_by_class(csv_file, output_name)
            else:
                print(f"‚ùå Fichier non trouv√©: {csv_file}")
        
        elif choice == '3':
            # Subset √©quilibr√©
            csv_file = input("\nChemin du fichier CSV: ").strip()
            output_name = input("Nom du subset: ").strip()
            samples = input("√âchantillons par classe (d√©faut: 1000): ").strip()
            
            samples_per_class = int(samples) if samples else 1000
            
            if Path(csv_file).exists():
                restructurer.create_balanced_subset(csv_file, output_name, samples_per_class)
            else:
                print(f"‚ùå Fichier non trouv√©: {csv_file}")
        
        elif choice == '4':
            # Lister
            restructurer.list_organized_datasets()
        
        elif choice == '5':
            print("\nüëã Au revoir!")
            break
        
        else:
            print("‚ùå Choix invalide")


if __name__ == "__main__":
    main()

